[2024-02-25 16:22:17,942][root][INFO] - Loading models for multi-turn dialogue for test...
INFO 02-25 16:22:22 llm_engine.py:70] Initializing an LLM engine with config: model='mistralai/Mistral-7B-Instruct-v0.2', tokenizer='mistralai/Mistral-7B-Instruct-v0.2', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir='/scr/andukuri/assistant-gate-hgx/pretrained_models/Mistral-7B-Instruct-v0.2', load_format=auto, tensor_parallel_size=2, quantization=None, enforce_eager=False, seed=0)
INFO 02-25 16:22:31 llm_engine.py:275] # GPU blocks: 61386, # CPU blocks: 4096
INFO 02-25 16:22:32 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 02-25 16:22:32 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.
[36m(RayWorkerVllm pid=2637819)[0m INFO 02-25 16:22:32 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
[36m(RayWorkerVllm pid=2637819)[0m INFO 02-25 16:22:32 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.
INFO 02-25 16:23:07 model_runner.py:547] Graph capturing finished in 35 secs.
[36m(RayWorkerVllm pid=2637819)[0m INFO 02-25 16:23:07 model_runner.py:547] Graph capturing finished in 35 secs.
[2024-02-25 16:23:07,652][root][INFO] - Beginning simulations for persona 0...
[2024-02-25 16:23:07,653][root][INFO] - Beginning simulations for persona 0, prompt batch 0...
[2024-02-25 16:23:10,244][root][INFO] - Beginning simulations for persona 1...
[2024-02-25 16:23:10,244][root][INFO] - Beginning simulations for persona 1, prompt batch 0...
[2024-02-25 16:23:12,788][root][INFO] - Beginning simulations for persona 2...
[2024-02-25 16:23:12,788][root][INFO] - Beginning simulations for persona 2, prompt batch 0...
[2024-02-25 16:23:15,502][root][INFO] - Beginning simulations for persona 3...
[2024-02-25 16:23:15,503][root][INFO] - Beginning simulations for persona 3, prompt batch 0...
[2024-02-25 16:23:17,947][root][INFO] - Beginning simulations for persona 4...
[2024-02-25 16:23:17,947][root][INFO] - Beginning simulations for persona 4, prompt batch 0...
[2024-02-25 16:23:20,594][root][INFO] - Beginning simulations for persona 5...
[2024-02-25 16:23:20,594][root][INFO] - Beginning simulations for persona 5, prompt batch 0...
[2024-02-25 16:23:23,328][root][INFO] - Beginning simulations for persona 6...
[2024-02-25 16:23:23,328][root][INFO] - Beginning simulations for persona 6, prompt batch 0...
[2024-02-25 16:23:26,014][root][INFO] - Beginning simulations for persona 7...
[2024-02-25 16:23:26,014][root][INFO] - Beginning simulations for persona 7, prompt batch 0...
[2024-02-25 16:23:28,544][root][INFO] - Beginning simulations for persona 8...
[2024-02-25 16:23:28,545][root][INFO] - Beginning simulations for persona 8, prompt batch 0...
[2024-02-25 16:23:31,110][root][INFO] - Beginning simulations for persona 9...
[2024-02-25 16:23:31,110][root][INFO] - Beginning simulations for persona 9, prompt batch 0...
[2024-02-25 16:23:40,416][root][INFO] - Loading models for multi-turn dialogue for test...
INFO 02-25 16:23:46 llm_engine.py:70] Initializing an LLM engine with config: model='mistralai/Mixtral-8x7B-Instruct-v0.1', tokenizer='mistralai/Mixtral-8x7B-Instruct-v0.1', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir='/scr/andukuri/assistant-gate-hgx/pretrained_models/Mixtral-8x7B-Instruct-v0.1', load_format=auto, tensor_parallel_size=2, quantization=None, enforce_eager=False, seed=0)
INFO 02-25 16:24:16 llm_engine.py:275] # GPU blocks: 21894, # CPU blocks: 4096
INFO 02-25 16:24:18 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 02-25 16:24:18 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.
[36m(RayWorkerVllm pid=2645871)[0m INFO 02-25 16:24:18 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
[36m(RayWorkerVllm pid=2645871)[0m INFO 02-25 16:24:18 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.
INFO 02-25 16:24:52 model_runner.py:547] Graph capturing finished in 35 secs.
[2024-02-25 16:24:52,774][root][INFO] - Beginning simulations for persona 0...
[36m(RayWorkerVllm pid=2645871)[0m INFO 02-25 16:24:52 model_runner.py:547] Graph capturing finished in 35 secs.
[2024-02-25 16:25:15,506][root][INFO] - Beginning simulations for persona 1...
[2024-02-25 16:25:38,559][root][INFO] - Beginning simulations for persona 2...
[2024-02-25 16:26:01,922][root][INFO] - Beginning simulations for persona 3...
[2024-02-25 16:26:24,247][root][INFO] - Beginning simulations for persona 4...
[2024-02-25 16:26:46,857][root][INFO] - Beginning simulations for persona 5...
[2024-02-25 16:27:08,272][root][INFO] - Beginning simulations for persona 6...
[2024-02-25 16:27:35,615][root][INFO] - Beginning simulations for persona 7...
[2024-02-25 16:27:58,121][root][INFO] - Beginning simulations for persona 8...
[2024-02-25 16:28:21,262][root][INFO] - Beginning simulations for persona 9...
[2024-02-25 16:28:53,850][root][INFO] - Loading models for multi-turn dialogue for test...
INFO 02-25 16:28:58 llm_engine.py:70] Initializing an LLM engine with config: model='mistralai/Mistral-7B-Instruct-v0.2', tokenizer='mistralai/Mistral-7B-Instruct-v0.2', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir='/scr/andukuri/assistant-gate-hgx/pretrained_models/Mistral-7B-Instruct-v0.2', load_format=auto, tensor_parallel_size=2, quantization=None, enforce_eager=False, seed=0)
INFO 02-25 16:29:07 llm_engine.py:275] # GPU blocks: 61386, # CPU blocks: 4096
INFO 02-25 16:29:08 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 02-25 16:29:08 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.
[36m(RayWorkerVllm pid=2654362)[0m INFO 02-25 16:29:08 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
[36m(RayWorkerVllm pid=2654362)[0m INFO 02-25 16:29:08 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.
INFO 02-25 16:29:43 model_runner.py:547] Graph capturing finished in 35 secs.
[36m(RayWorkerVllm pid=2654362)[0m INFO 02-25 16:29:43 model_runner.py:547] Graph capturing finished in 35 secs.
[2024-02-25 16:29:43,957][root][INFO] - Beginning simulations for persona 0...
[2024-02-25 16:29:52,043][root][INFO] - Beginning simulations for persona 1...
[2024-02-25 16:30:00,920][root][INFO] - Beginning simulations for persona 2...
[2024-02-25 16:30:09,672][root][INFO] - Beginning simulations for persona 3...
[2024-02-25 16:30:18,472][root][INFO] - Beginning simulations for persona 4...
[2024-02-25 16:30:27,607][root][INFO] - Beginning simulations for persona 5...
[2024-02-25 16:30:36,690][root][INFO] - Beginning simulations for persona 6...
[2024-02-25 16:30:45,103][root][INFO] - Beginning simulations for persona 7...
[2024-02-25 16:30:53,395][root][INFO] - Beginning simulations for persona 8...
[2024-02-25 16:31:01,555][root][INFO] - Beginning simulations for persona 9...
[2024-02-25 16:31:18,590][root][INFO] - Loading models for multi-turn dialogue for test...
INFO 02-25 16:31:23 llm_engine.py:70] Initializing an LLM engine with config: model='mistralai/Mixtral-8x7B-Instruct-v0.1', tokenizer='mistralai/Mixtral-8x7B-Instruct-v0.1', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir='/scr/andukuri/assistant-gate-hgx/pretrained_models/Mixtral-8x7B-Instruct-v0.1', load_format=auto, tensor_parallel_size=2, quantization=None, enforce_eager=False, seed=0)
INFO 02-25 16:31:42 llm_engine.py:275] # GPU blocks: 21894, # CPU blocks: 4096
INFO 02-25 16:31:44 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 02-25 16:31:44 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.
[36m(RayWorkerVllm pid=2662505)[0m INFO 02-25 16:31:44 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
[36m(RayWorkerVllm pid=2662505)[0m INFO 02-25 16:31:44 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.
INFO 02-25 16:32:19 model_runner.py:547] Graph capturing finished in 35 secs.
[2024-02-25 16:32:19,071][root][INFO] - Beginning simulations for persona 0...
[36m(RayWorkerVllm pid=2662505)[0m INFO 02-25 16:32:19 model_runner.py:547] Graph capturing finished in 35 secs.
[2024-02-25 16:32:59,491][root][INFO] - Beginning simulations for persona 1...
[2024-02-25 16:33:32,082][root][INFO] - Beginning simulations for persona 2...
[2024-02-25 16:34:08,107][root][INFO] - Beginning simulations for persona 3...
[2024-02-25 16:34:38,366][root][INFO] - Beginning simulations for persona 4...
[2024-02-25 16:35:09,463][root][INFO] - Beginning simulations for persona 5...
[2024-02-25 16:35:43,663][root][INFO] - Beginning simulations for persona 6...
[2024-02-25 16:36:16,673][root][INFO] - Beginning simulations for persona 7...
[2024-02-25 16:36:55,853][root][INFO] - Beginning simulations for persona 8...
[2024-02-25 16:37:27,482][root][INFO] - Beginning simulations for persona 9...
[2024-02-25 16:38:09,645][root][INFO] - Loading models for multi-turn dialogue for test...
INFO 02-25 16:38:14 llm_engine.py:70] Initializing an LLM engine with config: model='mistralai/Mistral-7B-Instruct-v0.2', tokenizer='mistralai/Mistral-7B-Instruct-v0.2', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir='/scr/andukuri/assistant-gate-hgx/pretrained_models/Mistral-7B-Instruct-v0.2', load_format=auto, tensor_parallel_size=2, quantization=None, enforce_eager=False, seed=0)
INFO 02-25 16:38:23 llm_engine.py:275] # GPU blocks: 61386, # CPU blocks: 4096
INFO 02-25 16:38:24 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 02-25 16:38:24 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.
[36m(RayWorkerVllm pid=2671269)[0m INFO 02-25 16:38:24 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
[36m(RayWorkerVllm pid=2671269)[0m INFO 02-25 16:38:24 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.
INFO 02-25 16:38:59 model_runner.py:547] Graph capturing finished in 35 secs.
[36m(RayWorkerVllm pid=2671269)[0m INFO 02-25 16:38:59 model_runner.py:547] Graph capturing finished in 35 secs.
[2024-02-25 16:38:59,780][root][INFO] - Beginning simulations for persona 0...
[2024-02-25 16:39:12,286][root][INFO] - Beginning simulations for persona 1...
[2024-02-25 16:39:24,716][root][INFO] - Beginning simulations for persona 2...
[2024-02-25 16:39:37,166][root][INFO] - Beginning simulations for persona 3...
[2024-02-25 16:39:52,825][root][INFO] - Beginning simulations for persona 4...
[2024-02-25 16:40:04,372][root][INFO] - Beginning simulations for persona 5...
[2024-02-25 16:40:16,705][root][INFO] - Beginning simulations for persona 6...
[2024-02-25 16:40:29,106][root][INFO] - Beginning simulations for persona 7...
[2024-02-25 16:40:40,497][root][INFO] - Beginning simulations for persona 8...
[2024-02-25 16:40:52,830][root][INFO] - Beginning simulations for persona 9...
[2024-02-25 16:41:13,275][root][INFO] - Loading models for multi-turn dialogue for test...
INFO 02-25 16:41:17 llm_engine.py:70] Initializing an LLM engine with config: model='mistralai/Mixtral-8x7B-Instruct-v0.1', tokenizer='mistralai/Mixtral-8x7B-Instruct-v0.1', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir='/scr/andukuri/assistant-gate-hgx/pretrained_models/Mixtral-8x7B-Instruct-v0.1', load_format=auto, tensor_parallel_size=2, quantization=None, enforce_eager=False, seed=0)
INFO 02-25 16:41:36 llm_engine.py:275] # GPU blocks: 21894, # CPU blocks: 4096
INFO 02-25 16:41:37 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 02-25 16:41:37 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.
[36m(RayWorkerVllm pid=2679639)[0m INFO 02-25 16:41:37 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
[36m(RayWorkerVllm pid=2679639)[0m INFO 02-25 16:41:37 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.
INFO 02-25 16:42:12 model_runner.py:547] Graph capturing finished in 35 secs.
[2024-02-25 16:42:12,518][root][INFO] - Beginning simulations for persona 0...
[36m(RayWorkerVllm pid=2679639)[0m INFO 02-25 16:42:12 model_runner.py:547] Graph capturing finished in 35 secs.
[2024-02-25 16:42:53,648][root][INFO] - Beginning simulations for persona 1...
[2024-02-25 16:43:37,730][root][INFO] - Beginning simulations for persona 2...
[2024-02-25 16:44:26,435][root][INFO] - Beginning simulations for persona 3...
[2024-02-25 16:45:17,118][root][INFO] - Beginning simulations for persona 4...
[2024-02-25 16:46:14,778][root][INFO] - Beginning simulations for persona 5...
[2024-02-25 16:46:57,224][root][INFO] - Beginning simulations for persona 6...
[2024-02-25 16:47:39,794][root][INFO] - Beginning simulations for persona 7...
[2024-02-25 16:48:27,947][root][INFO] - Beginning simulations for persona 8...
[2024-02-25 16:49:07,561][root][INFO] - Beginning simulations for persona 9...
