[2024-02-25 19:58:50,625][root][INFO] - Loading models for multi-turn dialogue for test...
INFO 02-25 19:58:55 llm_engine.py:70] Initializing an LLM engine with config: model='mistralai/Mistral-7B-Instruct-v0.2', tokenizer='mistralai/Mistral-7B-Instruct-v0.2', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir='/scr/andukuri/assistant-gate-hgx/pretrained_models/Mistral-7B-Instruct-v0.2', load_format=auto, tensor_parallel_size=2, quantization=None, enforce_eager=False, seed=0)
INFO 02-25 19:59:05 llm_engine.py:275] # GPU blocks: 61386, # CPU blocks: 4096
INFO 02-25 19:59:06 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 02-25 19:59:06 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.
[36m(RayWorkerVllm pid=2760846)[0m INFO 02-25 19:59:06 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
[36m(RayWorkerVllm pid=2760846)[0m INFO 02-25 19:59:06 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.
[36m(RayWorkerVllm pid=2760846)[0m INFO 02-25 19:59:41 model_runner.py:547] Graph capturing finished in 34 secs.
INFO 02-25 19:59:41 model_runner.py:547] Graph capturing finished in 35 secs.
[2024-02-25 19:59:41,553][root][INFO] - Beginning simulations for persona 0...
[2024-02-25 19:59:41,554][root][INFO] - Beginning simulations for persona 0, prompt batch 0...
[2024-02-25 19:59:45,937][root][INFO] - Beginning simulations for persona 1...
[2024-02-25 19:59:45,937][root][INFO] - Beginning simulations for persona 1, prompt batch 0...
[2024-02-25 19:59:49,915][root][INFO] - Beginning simulations for persona 2...
[2024-02-25 19:59:49,916][root][INFO] - Beginning simulations for persona 2, prompt batch 0...
[2024-02-25 19:59:54,257][root][INFO] - Beginning simulations for persona 3...
[2024-02-25 19:59:54,257][root][INFO] - Beginning simulations for persona 3, prompt batch 0...
[2024-02-25 19:59:58,271][root][INFO] - Beginning simulations for persona 4...
[2024-02-25 19:59:58,271][root][INFO] - Beginning simulations for persona 4, prompt batch 0...
[2024-02-25 20:00:03,173][root][INFO] - Beginning simulations for persona 5...
[2024-02-25 20:00:03,173][root][INFO] - Beginning simulations for persona 5, prompt batch 0...
[2024-02-25 20:00:07,102][root][INFO] - Beginning simulations for persona 6...
[2024-02-25 20:00:07,103][root][INFO] - Beginning simulations for persona 6, prompt batch 0...
[2024-02-25 20:00:11,652][root][INFO] - Beginning simulations for persona 7...
[2024-02-25 20:00:11,653][root][INFO] - Beginning simulations for persona 7, prompt batch 0...
[2024-02-25 20:00:15,739][root][INFO] - Beginning simulations for persona 8...
[2024-02-25 20:00:15,739][root][INFO] - Beginning simulations for persona 8, prompt batch 0...
[2024-02-25 20:00:20,161][root][INFO] - Beginning simulations for persona 9...
[2024-02-25 20:00:20,162][root][INFO] - Beginning simulations for persona 9, prompt batch 0...
[2024-02-25 20:00:31,274][root][INFO] - Loading models for multi-turn dialogue for test...
INFO 02-25 20:00:35 llm_engine.py:70] Initializing an LLM engine with config: model='mistralai/Mixtral-8x7B-Instruct-v0.1', tokenizer='mistralai/Mixtral-8x7B-Instruct-v0.1', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir='/scr/andukuri/assistant-gate-hgx/pretrained_models/Mixtral-8x7B-Instruct-v0.1', load_format=auto, tensor_parallel_size=2, quantization=None, enforce_eager=False, seed=0)
INFO 02-25 20:00:54 llm_engine.py:275] # GPU blocks: 21894, # CPU blocks: 4096
INFO 02-25 20:00:56 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 02-25 20:00:56 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.
[36m(RayWorkerVllm pid=2768659)[0m INFO 02-25 20:00:56 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
[36m(RayWorkerVllm pid=2768659)[0m INFO 02-25 20:00:56 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.
INFO 02-25 20:01:30 model_runner.py:547] Graph capturing finished in 35 secs.
[2024-02-25 20:01:30,995][root][INFO] - Beginning simulations for persona 0...
[36m(RayWorkerVllm pid=2768659)[0m INFO 02-25 20:01:30 model_runner.py:547] Graph capturing finished in 35 secs.
[2024-02-25 20:02:12,933][root][INFO] - Beginning simulations for persona 1...
[2024-02-25 20:02:59,235][root][INFO] - Beginning simulations for persona 2...
[2024-02-25 20:03:43,329][root][INFO] - Beginning simulations for persona 3...
[2024-02-25 20:04:29,618][root][INFO] - Beginning simulations for persona 4...
[2024-02-25 20:05:16,017][root][INFO] - Beginning simulations for persona 5...
[2024-02-25 20:06:01,729][root][INFO] - Beginning simulations for persona 6...
[2024-02-25 20:06:46,653][root][INFO] - Beginning simulations for persona 7...
[2024-02-25 20:07:31,808][root][INFO] - Beginning simulations for persona 8...
[2024-02-25 20:08:16,364][root][INFO] - Beginning simulations for persona 9...
[2024-02-25 20:09:09,548][root][INFO] - Loading models for multi-turn dialogue for test...
INFO 02-25 20:09:14 llm_engine.py:70] Initializing an LLM engine with config: model='mistralai/Mistral-7B-Instruct-v0.2', tokenizer='mistralai/Mistral-7B-Instruct-v0.2', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir='/scr/andukuri/assistant-gate-hgx/pretrained_models/Mistral-7B-Instruct-v0.2', load_format=auto, tensor_parallel_size=2, quantization=None, enforce_eager=False, seed=0)
INFO 02-25 20:09:23 llm_engine.py:275] # GPU blocks: 61386, # CPU blocks: 4096
INFO 02-25 20:09:24 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 02-25 20:09:24 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.
[36m(RayWorkerVllm pid=2777573)[0m INFO 02-25 20:09:24 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
[36m(RayWorkerVllm pid=2777573)[0m INFO 02-25 20:09:24 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.
INFO 02-25 20:09:59 model_runner.py:547] Graph capturing finished in 35 secs.
[36m(RayWorkerVllm pid=2777573)[0m INFO 02-25 20:09:59 model_runner.py:547] Graph capturing finished in 35 secs.
[2024-02-25 20:09:59,277][root][INFO] - Beginning simulations for persona 0...
[2024-02-25 20:10:15,650][root][INFO] - Beginning simulations for persona 1...
[2024-02-25 20:10:30,312][root][INFO] - Beginning simulations for persona 2...
[2024-02-25 20:10:46,419][root][INFO] - Beginning simulations for persona 3...
[2024-02-25 20:11:02,078][root][INFO] - Beginning simulations for persona 4...
[2024-02-25 20:11:17,907][root][INFO] - Beginning simulations for persona 5...
[2024-02-25 20:11:34,867][root][INFO] - Beginning simulations for persona 6...
[2024-02-25 20:11:51,837][root][INFO] - Beginning simulations for persona 7...
[2024-02-25 20:12:06,135][root][INFO] - Beginning simulations for persona 8...
[2024-02-25 20:12:22,038][root][INFO] - Beginning simulations for persona 9...
[2024-02-25 20:12:45,230][root][INFO] - Loading models for multi-turn dialogue for test...
INFO 02-25 20:12:49 llm_engine.py:70] Initializing an LLM engine with config: model='mistralai/Mixtral-8x7B-Instruct-v0.1', tokenizer='mistralai/Mixtral-8x7B-Instruct-v0.1', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir='/scr/andukuri/assistant-gate-hgx/pretrained_models/Mixtral-8x7B-Instruct-v0.1', load_format=auto, tensor_parallel_size=2, quantization=None, enforce_eager=False, seed=0)
INFO 02-25 20:13:08 llm_engine.py:275] # GPU blocks: 21894, # CPU blocks: 4096
INFO 02-25 20:13:09 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 02-25 20:13:09 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.
[36m(RayWorkerVllm pid=2785714)[0m INFO 02-25 20:13:09 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
[36m(RayWorkerVllm pid=2785714)[0m INFO 02-25 20:13:09 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.
INFO 02-25 20:13:44 model_runner.py:547] Graph capturing finished in 35 secs.
[2024-02-25 20:13:44,237][root][INFO] - Beginning simulations for persona 0...
[36m(RayWorkerVllm pid=2785714)[0m INFO 02-25 20:13:44 model_runner.py:547] Graph capturing finished in 35 secs.
[2024-02-25 20:14:54,256][root][INFO] - Beginning simulations for persona 1...
[2024-02-25 20:15:52,456][root][INFO] - Beginning simulations for persona 2...
[2024-02-25 20:16:59,750][root][INFO] - Beginning simulations for persona 3...
[2024-02-25 20:18:05,425][root][INFO] - Beginning simulations for persona 4...
[2024-02-25 20:19:07,975][root][INFO] - Beginning simulations for persona 5...
[2024-02-25 20:20:09,753][root][INFO] - Beginning simulations for persona 6...
[2024-02-25 20:21:17,537][root][INFO] - Beginning simulations for persona 7...
[2024-02-25 20:22:16,706][root][INFO] - Beginning simulations for persona 8...
[2024-02-25 20:23:38,760][root][INFO] - Beginning simulations for persona 9...
[2024-02-25 20:24:50,503][root][INFO] - Loading models for multi-turn dialogue for test...
INFO 02-25 20:24:55 llm_engine.py:70] Initializing an LLM engine with config: model='mistralai/Mistral-7B-Instruct-v0.2', tokenizer='mistralai/Mistral-7B-Instruct-v0.2', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir='/scr/andukuri/assistant-gate-hgx/pretrained_models/Mistral-7B-Instruct-v0.2', load_format=auto, tensor_parallel_size=2, quantization=None, enforce_eager=False, seed=0)
INFO 02-25 20:25:04 llm_engine.py:275] # GPU blocks: 61386, # CPU blocks: 4096
INFO 02-25 20:25:05 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 02-25 20:25:05 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.
[36m(RayWorkerVllm pid=2799332)[0m INFO 02-25 20:25:05 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
[36m(RayWorkerVllm pid=2799332)[0m INFO 02-25 20:25:05 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.
INFO 02-25 20:25:40 model_runner.py:547] Graph capturing finished in 35 secs.
[36m(RayWorkerVllm pid=2799332)[0m INFO 02-25 20:25:40 model_runner.py:547] Graph capturing finished in 35 secs.
[2024-02-25 20:25:40,465][root][INFO] - Beginning simulations for persona 0...
[2024-02-25 20:26:03,444][root][INFO] - Beginning simulations for persona 1...
[2024-02-25 20:26:24,333][root][INFO] - Beginning simulations for persona 2...
[2024-02-25 20:26:50,106][root][INFO] - Beginning simulations for persona 3...
[2024-02-25 20:27:13,694][root][INFO] - Beginning simulations for persona 4...
[2024-02-25 20:27:36,667][root][INFO] - Beginning simulations for persona 5...
[2024-02-25 20:28:01,436][root][INFO] - Beginning simulations for persona 6...
[2024-02-25 20:28:23,958][root][INFO] - Beginning simulations for persona 7...
[2024-02-25 20:28:47,304][root][INFO] - Beginning simulations for persona 8...
[2024-02-25 20:29:14,452][root][INFO] - Beginning simulations for persona 9...
[2024-02-25 20:29:48,314][root][INFO] - Loading models for multi-turn dialogue for test...
INFO 02-25 20:29:53 llm_engine.py:70] Initializing an LLM engine with config: model='mistralai/Mixtral-8x7B-Instruct-v0.1', tokenizer='mistralai/Mixtral-8x7B-Instruct-v0.1', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir='/scr/andukuri/assistant-gate-hgx/pretrained_models/Mixtral-8x7B-Instruct-v0.1', load_format=auto, tensor_parallel_size=2, quantization=None, enforce_eager=False, seed=0)
INFO 02-25 20:30:10 llm_engine.py:275] # GPU blocks: 21894, # CPU blocks: 4096
INFO 02-25 20:30:11 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 02-25 20:30:11 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.
[36m(RayWorkerVllm pid=2807627)[0m INFO 02-25 20:30:11 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
[36m(RayWorkerVllm pid=2807627)[0m INFO 02-25 20:30:11 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.
[36m(RayWorkerVllm pid=2807627)[0m INFO 02-25 20:30:46 model_runner.py:547] Graph capturing finished in 35 secs.
INFO 02-25 20:30:46 model_runner.py:547] Graph capturing finished in 35 secs.
[2024-02-25 20:30:46,860][root][INFO] - Beginning simulations for persona 0...
[2024-02-25 20:32:16,245][root][INFO] - Beginning simulations for persona 1...
[2024-02-25 20:33:32,488][root][INFO] - Beginning simulations for persona 2...
[2024-02-25 20:35:25,095][root][INFO] - Beginning simulations for persona 3...
[2024-02-25 20:36:49,889][root][INFO] - Beginning simulations for persona 4...
[2024-02-25 20:38:16,543][root][INFO] - Beginning simulations for persona 5...
[2024-02-25 20:39:50,683][root][INFO] - Beginning simulations for persona 6...
[2024-02-25 20:41:46,448][root][INFO] - Beginning simulations for persona 7...
[2024-02-25 20:43:11,021][root][INFO] - Beginning simulations for persona 8...
[2024-02-25 20:44:47,783][root][INFO] - Beginning simulations for persona 9...
